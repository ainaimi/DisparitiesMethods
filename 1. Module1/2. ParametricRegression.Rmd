---
title: "Introduction to Regression Using R"
author: "Ashley I Naimi"
date: "Dec 2022"
urlcolor: blue
bibliography: ref.bib
link-citations: yes
output: 
    bookdown::pdf_book:
      base_format: tint::tintPdf
      toc: true
      number_sections: true
      includes:
        in_header: "../misc/preamble.tex"
      latex_engine: xelatex
    html_document:
      theme: readable
      toc: true
      toc_float: true
      number_sections: true
      css: "../misc/style.css"
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=40),tidy=TRUE)

packages <- c( "data.table","tidyverse","ggplot2","ggExtra","formatR",
               "gridExtra","skimr","here","Hmisc","RColorBrewer",
               "broom","boot")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN',dependencies=T)
  }
}

for (package in packages) {
  library(package, character.only=T)
}

remotes::install_github("rstudio/fontawesome")

library(fontawesome)

thm <- theme_classic() +
  theme(
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

\newpage
\onehalfspacing

\newpage
\onehalfspacing

# Introduction to Regression with R

In this section, we'll cover how to implement generalized linear models in R. We will not cover the theory behind GLMs, but rather focus on the functions in the R programming language that will enable us to implement regression models for binary and continuous data. 

There are several types of regression models that can be fit to data. Below is a table with different regression modeling strategies, and their corresponding packages or functions in R:

| Regression Type  | R Package or Function |
|---|---|
| Generalized Linear Models       | GLM (base R) |
| Multinomial/Ordinal/Polytomous  | `VGAM`, `nnet` |
| Quantile Regression             | `quantreg`  |
| Cox PH Regression               | `survival`   |
| Accelerated Failure Time        | `survival`, `flexsurv`   |

Here, we will explore the `glm()` function in base R, and how it can be used to estimate exposure-outcome associations in different ways^[Specifically, outcome modeling and propensity score modeling.]. 

# The Data: NHEFS

To explore the use of the `glm` function in R, we'll rely on the NHANES Epidemiologic Follow-Up Survey data.^[Details on these data can be found on the "Causal Inference: What If?" book website: [https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)] These data will be used to explore the association between quitting smoking and change in weight between 1971 and 1982. Let's start by loading and exploring the data: 

```{r, message=F, warning=F}
nhefs <- read_csv(here("data","nhefs.csv")) %>% 
  mutate(wt_delta = as.numeric(wt82_71>0))

#' Quick view of data
dim(nhefs)

names(nhefs)

head(nhefs)
```

To start, we'll use these data to fit two regression models: one where the outcome is continuous weight change (`wt82_71`), and one where the outcome is binary weight change (`wt_delta`). Let's start with the continuous outcome, and seek to estimate the association between quitting smoking and weight change, adjusted for several variables. 

## Continuous Outcome

The most common approach for deploying a glm would be as follows:

```{r, warning = F, message = F}
#' Here, we start fitting relevant regression models to the data.

model1 <- glm(wt82_71 ~ qsmk + sex + age + income + sbp + dbp + price71 + tax71 + race,
              data=nhefs,
              family = gaussian("identity"))

```

In the above code, we've fit the GLM to our data, and have stored the fit of our model to an object we call `model1`. Let's explore what's in this model using the `list` function:

```{r, warning = F, message = F}
ls(model1)
```

We can extract the information we want from the model fit using a few standard functions. For example, the output one would obtain from other software programs such as SAS or Stata can be printed to the console in R using the summary function:

```{r, warning = F, message = F}

summary(model1)

```

Sometimes, we may be interested in storing this model output as a dataset, or saving particular elements from the summary. There are functions in the `broom` package that allow us to do this easily:

```{r, warning = F, message = F}

library(broom)

tidy(model1)

```

Oftentimes, we need to extract additional information from the GLM fit, such as model residuals, fitted values, or predictions. This is easy in R. For example, we can extract the residuals from the above model:

```{r, warning = F, message = F}

model1_residuals <- tibble(residuals = model1$residuals,
                           index = 1:length(model1$residuals))

plot1 <- ggplot(model1_residuals) +
  geom_point(aes(y=residuals, x = index))

ggsave(here("figures","residual_plot.pdf"))

```

```{r residualplot, out.width="10cm", fig.align='center', fig.margin=FALSE, echo=F, fig.cap="Plot of residuals from model1."}
knitr::include_graphics(here("figures","residual_plot.pdf"))
```

We may also be interested in obtaining predictions from `model1`. There are different types of predictions we may want to obtain. For example, we may want predictions under the observed values of all the variables in the dataset. These are often referred to as fitted values, which we can get using the following:

```{r, warning = F, message = F}

model1_fitted <- tibble(fitted_values = model1$fitted.values,
                        index = 1:length(model1$fitted.values))

head(model1_fitted)

```

Alternatively, we can use the `predict` function to obtain fitted values under the observed data, or under values other than the observed data^[We will see how we can get predictions under different observed data values in the subsequent section.].

```{r, warning = F, message = F}

model1_fitted <- tibble(fitted_values = model1$fitted.values,
                        fitted_values2 = predict(model1, newdata = nhefs),
                        index = 1:length(model1$fitted.values))

head(model1_fitted)

plot1 <- ggplot(model1_fitted) +
  geom_point(aes(y=fitted_values, x = fitted_values2))

ggsave(here("figures","fitted_plot.pdf"))

```

```{r fittedplot, out.width="10cm", fig.align='center', fig.margin=FALSE, echo=F, fig.cap="Plot of fitted values from model1 obtained from the glm function and the predict function."}
knitr::include_graphics(here("figures","fitted_plot.pdf"))
```

:::{.rmdnote data-latex="{tip}"}

__Exercise 1__:

|               Model residuals are typically calculated as the predicted minus the observed values. Can you generate a figure that plots the residuals obtained from `model1` above against residuals that you create using the `predict()` function in R?

:::

## Binary Outcome

When our outcome is binary, we may be interested in estimating the risk difference, risk ratio, or odds ratio for the exposure-outcome association. We can estimate each of these using GLMs but with different link functions and distributions.

We'll focus on the odds ratio first:

```{r, warning = F, message = F}
#' Here, we start fitting relevant regression models to the data.

model1 <- glm(wt_delta ~ qsmk + sex + age + income + sbp + dbp + price71 + tax71 + race,
              data=nhefs,
              family = binomial("logit"))

```

We can get the odds ratio from this model using the following code:

```{r, warning = F, message = F}

model1_OR <- tidy(model1)[2,] %>% 
  mutate(OR = exp(estimate),
         LCL = exp(estimate - 1.96*std.error),
         UCL = exp(estimate + 1.96*std.error)) %>% 
  select(term, OR, LCL, UCL)

model1_OR

```

We can now take the results we obtained from the `model1_OR` object and incorporate them into our manuscript or report. For example, if you are writing a report in Microsoft Word, you may opt to save these results as an Excel spreadsheet: 

```{r}

write_excel_csv(model1_OR, file = here("misc", "OddsRatioResults.csv"))

```

Alternatively, if you are using a Markdown language such as RMarkdown, you can print the table directly with, for example, the `kable` package:

```{r}
knitr::kable(model1_OR)
```
&nbsp;
There are many stylng themes for a `kable` table, and you can find additional information on `kable` and the themes available [here](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html).

# Estimating Risk Differences and Ratios with GLM

Generalized linear models consist of a family of regression models that are fully characterized by a selected distribution and a link function. That is, to fully specify a GLM, one must select a distribution (which determines the form of the conditional mean and variance of the outcome) and a link function (which determines how the conditional mean of the outcome relates to the covariates).

There are a wide variety of distributions and link functions available in standard statistical software programs that fit GLMs. Here, we'll consider a binary outcome $Y$ with probability $P(Y=1)$, and focus attention on three link functions: 

1. Logit, or the log-odds: $\log{P(Y=1)/[1-P(Y=1)]}$

We saw above how to use R's GLM function to fit a logistic regression model.

2. Log: $\log[P(Y=1)]$
3. Identity: $P(Y=1)$.

A common misconception is that to use GLMs correctly, one must choose the distribution that best characterizes the data, as well as the canonical link function corresponding to this distribution. For example, if the outcome is binary, one "must" choose the binomial distribution with the logit link. 

While the binomial distribution and logit link work well together for binary outcomes, they do not easily provide contrasts like the risk difference or risk ratio, because of the selected link function. Alternative specification of the distribution and link function for GLMs can address this limitation.

## Link Functions and Effect Measures

There is an important relation between the chosen link function, and the interpretation of the coefficients from a GLM. For models of a binary outcome and the logit or log link, this relation stems from the properties and rules governing the natural logarithm. Specifically, the quotient rule for logarithms states that:

$$\log(X/Y) = \log(X) − \log(Y)$$
Because of this relation, the natural exponent of the coefficient in a logistic regression model yields an estimate of the odds ratio. However, by the same reasoning, exponentiating the coefficient from a GLM with a log link function and a binomial distribution (i.e., log-binomial regression) yields an estimate of the risk ratio. 

Alternately, for GLM models with a binomial distribution and identity link function, because logarithms are not used, the unexponentiated coefficient yields an estimate of the risk difference.

Unfortunately, using a binomial distribution can lead to convergence problems with the $\log()$ or identity link functions for reasons that have been explored [@Zou2004].

This will occur when, for example, the combined numerical value of all the independent variables in the model is very large. This can result in estimated probabilities that exceed 1, which violates the definition of a probability (binomial) model (probabilities can only lie between zero and one) and hence, convergence problems.

Let's see how these problems can be overcome.

## GLMs for risk differences and ratios

For our analyses of the data described above using GLM with a binomial distributed outcome with a log link function to estimate the risk ratio and identity link function to estimate risk difference, an error is returned:

```{r, error = T}
#' Here, we start fitting relevant regression models to the data.
#' modelForm is a regression argument that one can use to regress the 
#' outcome (wt_delta) against the exposure (qsmk) and selected confounders.

#' This model can be used to quantify a conditionally adjusted risk 
#' ratio with with correct standard error
#' However, error it returns an error and thus does not provide any results.

modelRR_binom <- glm(wt_delta ~ qsmk + sex + age + income + sbp + dbp + price71 + tax71 + race, data=nhefs, family = binomial("log"))

```

Why is this error returned? The most likely explanation in this context is as follows: We are modeling $P(Y = 1 \mid X) = \exp\{\beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p\}$. In this context, there may be *no set of values* for the parameters in the model that yield $P(Y = 1 \mid X) < 1$ for every observation in the sample. Because R's glm function (under a binomial distribution) correctly recognizes this as a problem, it returns an error.

Instead, one may resort to using different distributions that are more compatible with the link functions that return the association measures of interest. For the risk ratio, one may use a GLM with a Poisson distribution and log link function. Doing so will return an exposure coefficient whose natural exponent can be interpreted as a risk ratio. 

```{r}
#' This model can be used to quantify a conditionally risk ratio 
#' using the Poisson distribution and log link function. 
#' However, because the Poisson distribution is used, the model 
#' provides incorrect standard error estimates.
modelRR <- glm(wt_delta ~ qsmk + sex + age + income + sbp + dbp + price71 + tax71 + race, data=nhefs, family = poisson("log"))
tidy(modelRR)[2,]
```

It's important to recognize what we're doing here. We are using this model as a tool to quantify the log mean ratio contrasting $P(Y = 1 \mid X_{qsmk} = 1)$ to $P(Y = 1 \mid X_{qsmk} = 0)$ (all other things being equal). However, we should not generally assume that ever aspect of this model is correct. In particular, note that the max predicted probability from this model is `r round(max(modelRR$fitted.values), 3)`:

```{r, message=F, warning=F}
summary(modelRR$fitted.values)
```

We can use the `augment` function in the `broom` package to evaluate the distribution of these probabilities (among other things):
```{r, message=F, warning=F}
fitted_dat <- augment(modelRR, type.predict="response")

fitted_dat

plot_hist <- ggplot(fitted_dat) +
  geom_histogram(aes(.fitted)) +
  scale_y_continuous(expand=c(0,0)) +
  scale_x_continuous(expand=c(0,0))

ggsave(here("figures","2022_02_21-rr_hist_plot.pdf"), plot=plot_hist)
```

This distribution is shown in margin Figure \ref{fig:fittedhist}. We can also see that there are only two observations in the sample with predicted risks greater than 1.

```{r fittedhist, out.width="5cm", fig.align='center', fig.margin=TRUE, echo=F, fig.cap="Distribution of fitted values from the Poisson GLM with log link function to obtain an estimate of the adjusted risk ratio for the association between quitting smoking and greater than median weight gain in the NHEFS."}
knitr::include_graphics(here("figures","2022_02_21-rr_hist_plot.pdf"))
```

```{r}
fitted_dat %>% 
  filter(.fitted >= 1) %>% 
  select(wt_delta, qsmk, age, .fitted)
```

For these reasons, we are not particularly concerned about the fact that the model predicts risks that are slightly large than 1. However, the model-based standard errors (i.e., the SEs that one typically obtains directly from the GLM output) are no longer valid. Instead, one should use the robust (or sandwich) variance estimator to obtain valid SEs (the bootstrap can also be used) [@Zou2004].

```{r}
library(sandwich)
#' To obtain the correct variance, we use the "sandwich" 
#' function to obtain correct sandwich (robust) standard 
#' error estimates.
sqrt(sandwich(modelRR)[2,2])
```

Another way to obtain robust (sandwich) variance estimates that are accurate is to use the `coeftest` function in the `lmtest` package. To use this function, we take the model object created, and generate a variance covariance matrix for the model parameters using the `vcovHC` function. We then use this variance-covariance matrix as an argument in the `coeftest` function to obtain results from our risk ratio model with appropriate standard errors.

```{r}
library(sandwich)
library(lmtest)

# first create sandwich vcov matrix
vcov_RR <- vcovHC(modelRR, type="HC3", sandwich=T)

# then construct summary results using sandwich vcov matrix
coeftest(modelRR, vcov = vcov_RR, type = "HC3")

# compare these results to model-based results
summary(modelRR)


```


For the risk difference, one may use a GLM with a Gaussian (i.e., normal) distribution and identity link function, or, equivalently, an ordinary least squares estimator. Doing so will return an exposure coefficient that can be interpreted as a risk difference. However, once again the robust variance estimator (or bootstrap) should be used to obtain valid SEs.

```{r}
#' This model can be used to obtain a risk difference 
#' with the gaussian distribiton or using ordinary least 
#' squares (OLS, via the lm function). Again, the model 
#' based standard error estimates are incorrect. 
modelRD <- glm(wt_delta ~ qsmk + sex + age + income + sbp + dbp + price71 + tax71 + race, data=nhefs,family = gaussian("identity"))
modelRD <- lm(wt_delta ~ qsmk + sex + age + income + sbp + dbp + price71 + tax71 + race, data=nhefs)
tidy(modelRD)[2,]
#' To obtain the correct variance, we use the "sandwich" function
#' to obtain correct sandwich (robust) standard error estimates.
sqrt(sandwich(modelRD)[2,2])
```

The risk ratio and difference, as well as the 95% sandwich variance confidence intervals, obtained for the relation between quitting smoking and greater than median weight change are provided Table 1.

```{r, echo = F}
table1_data <- tibble(Method = c("GLM","Marginal Standardization"),
                      `Risk Difference` = c("0.14	(0.09, 0.20)", "0.14	(0.09, 0.21)"), 
                      `Risk Ratio` = c("1.32	(1.19, 1.46)", "1.31	(1.18, 1.46)"))
```

```{r}
knitr::kable(table1_data)
```


Results in this table obtained using a conditionally adjusted regression model without interactions. Gaussian distribution and identity link was used to obtain the risk difference. A Poisson distribution and log link was used to obtain the risk ratio. 95% CIs obtained via the sandwich variance estimator. 95% CIs obtained using the bias-corrected and accelerated bootstrap CI estimator.

Unfortunately, use of a Poisson or Gaussian distribution for GLMs for a binomial outcome can introduce different problems. For one, while not entirely worrysome in our setting, a model that predicts probabilities greater than one should not instill confidence in the user. Second, performance of the robust variance estimator is notoriously poor with small sample sizes. Finally, the interpretation of the risk differences and ratios becomes more complex when the exposure interacts with other variables in the model. 

```{r, echo = F}
table2_data <- tibble(`Odds Ratio` = c("GLM Family = Binomial", "GLM Link = Logistic", "Standard Errors = Model Based", " ", " ", " ", " ", " "), 
                      `Risk Ratio` = c("GLM Family = Binomial", "GLM Link = Log", "Standard Errors = Model Based", "GLM Family = Poisson", "GLM Link = Log", "Standard Errors = Sandwich", " ", " "),
                      `Risk Difference` = c("GLM Family = Binomial", "GLM Link = Identity", "Standard Errors = Model Based", "GLM Family = Gaussian", "GLM Link = Identity", "Standard Errors = Sandwich", "Least Squares Regression", "Standard Errors = Sandwich"))
```

```{r, echo=F}
knitr::kable(table2_data, "simple", caption = "Methods to use for quantifying conditionally adjusted odds ratios, risk ratios, and risk differences.")
```

For instance, let's assume that in the NHEFS data, the association between quitting smoking and weight gain interacts with race:

```{r}
#' Potential evidence for interaction 
#' between smoking and exercise on the risk difference scale?

table(nhefs$race)

summary(glm(wt_delta ~ qsmk + sex + age + income + sbp + dbp + price71 + tax71 + race, data=nhefs, family=binomial(link="identity")))
summary(glm(wt_delta ~ qsmk + sex + age + income + sbp + dbp + price71 + tax71 + race + qsmk*race, data=nhefs, family=binomial(link="identity")))
```

If this were the case, to properly interpret the association, the interaction between race and qsmk should be considered. But how could we do this? One approach would be to include the interaction term and interpret the association between quitting smoking and weight change separately for each racial category.

For example, in the model that includes the interaction term with exercise, we can no longer simply interpret the coefficient for `qsmk` as the treatment effet of interest. Instead, (under causal identifiability) we have two treatment effects: The effect of `qsmk` for those with `race = 0` and `1`. If we were, in fact, interested in the average treatment effect in the sample (and not a unique treatment effect for each racial group), we would have to take a weighted average of the coefficients for these effects, where the weights are defined as a function of the proportion of individuals in each racial category. 

Clearly, this approach can quickly become too burdensome when there are several relevant interactions in the model, and is not worth the effort when we are simply interested in the marginal association. As an alternative, we can use marginal or model-based standardization, which can greatly simplify the process.

## Marginal or Model-Based Standardization

Another approach to obtaining risk differences and ratios from GLMs that are not subject to the limitations noted above is to use marginal standardization, which is equivalent to g computation when the exposure is measured at a single time point [@Naimi2016b]. This process can be implemented by fitting a single logistic model, regressing the binary outcome against all confounder variables, including all relevant interactions. But instead of reading the coefficients the model, one can obtain odds ratios, risk ratios, or risk differences by using this model to generate predicted risks for each individual under “exposed” and “unexposed” scenarios in the dataset. To obtain standard errors, the entire procedure must be bootstrapped (see supplemental material for code). These marginal risk differences and ratios, as well as their bootstrapped CIs are presented in the table above.

Here is some code to implement this marginal standardization in the NHEFS data:

```{r}
#' Marginal Standardization: version 1
#' Regress the outcome against the confounders with interaction
ms_model <- glm(wt_delta ~ qsmk + sex + age + income + sbp + dbp + price71 + tax71 + race + race*qsmk, data=nhefs, family=binomial("logit"))
##' Generate predictions for everyone in the sample to obtain 
##' unexposed (mu0 predictions) and exposed (mu1 predictions) risks.
mu1 <- predict(ms_model,newdata=transform(nhefs,qsmk=1),type="response")
mu0 <- predict(ms_model,newdata=transform(nhefs,qsmk=0),type="response")

#' Marginally adjusted odds ratio
marg_stand_OR <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
#' Marginally adjusted risk ratio
marg_stand_RR <- mean(mu1)/mean(mu0)
#' Marginally adjusted risk difference
marg_stand_RD <- mean(mu1)-mean(mu0)

#' Using the bootstrap to obtain confidence intervals for the marginally adjusted 
#' risk ratio and risk difference.
bootfunc <- function(data,index){
  boot_dat <- data[index,]
  ms_model <- glm(wt_delta ~ qsmk + sex + age + income + sbp + dbp + price71 + tax71 + race + race*qsmk,data=boot_dat,family=binomial("logit"))
  mu1 <- predict(ms_model,newdata=transform(boot_dat,qsmk=1),type="response")
  mu0 <- predict(ms_model,newdata=transform(boot_dat,qsmk=0),type="response")
  
  marg_stand_OR_ <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
  marg_stand_RR_ <- mean(mu1)/mean(mu0)
  marg_stand_RD_ <- mean(mu1)-mean(mu0)
  res <- c(marg_stand_RD_,marg_stand_RR_,marg_stand_OR_)
  return(res)
}

#' Run the boot function. Set a seed to obtain reproducibility
set.seed(123)
boot_res <- boot(nhefs,bootfunc,R=2000)

boot_RD <- boot.ci(boot_res,index=1)
boot_RR <- boot.ci(boot_res,index=2)
boot_OR <- boot.ci(boot_res,index=3)

marg_stand_OR
marg_stand_RR
marg_stand_RD

boot_RD
boot_RR
boot_OR

```

While this marginal standardization approach is more flexible in that it accounts for the interaction between quitting smoking and race, and still yields an estimate of the average treatment effect (again, under identifiability), it still assumes a constant effect of qsmk on weight change across levels of all of the other variables in the model. This constant effect assumption might be true, but if one wanted to account for potential interactions between the exposure and all of the confounders in the model, there is an easy way. We call this the "stratified modeling approach."

This stratified modeling approach avoids the exposure effect homogeneity assumption across levels of all the confounders. In effect, the approach fits a separate model for each exposure stratum. To obtain predictions under the “exposed” scenario, we use the model fit to the exposed individuals to generate predicted outcomes in the entire sample. To obtain predictions under the “unexposed” scenario, we repeat the same procedure, but with the model fit among the unexposed. One can then average the risks obtained under each exposure scenario, and take their difference and ratio to obtain the risk differences and ratios of interest.
```{r}
#' Marginal Standardization
##' To avoid assuming no interaction between 
##' smoking and any of the other variables
##' in the model, we subset modeling among 
##' exposed/unexposed. This code removes smoking from the model,
##' which will allow us to regress the outcome 
##' against the confounders among the exposed and 
##' the unexposed searately. Doing so will allow us 
##' to account for any potential exposure-covariate interactions
##' that may be present. 

#' Regress the outcome against the confounders 
#' among the unexposed (model0) and then among the exposed (model1)
model0 <- glm(wt_delta ~ sex + age + income + sbp + dbp + price71 + tax71 + race, data=subset(nhefs,qsmk==0), family=binomial("logit"))
model1 <- glm(wt_delta ~ sex + age + income + sbp + dbp + price71 + tax71 + race, data=subset(nhefs,qsmk==1), family=binomial("logit"))
##' Generate predictions for everyone in the sample using the model fit to only the 
##' unexposed (mu0 predictions) and only the exposed (mu1 predictions).
mu1 <- predict(model1,newdata=nhefs,type="response")
mu0 <- predict(model0,newdata=nhefs,type="response")

#' Marginally adjusted odds ratio
marg_stand_OR <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
#' Marginally adjusted risk ratio
marg_stand_RR <- mean(mu1)/mean(mu0)
#' Marginally adjusted risk difference
marg_stand_RD <- mean(mu1)-mean(mu0)

#' Using the bootstrap to obtain confidence intervals for the marginally adjusted 
#' risk ratio and risk difference.
bootfunc <- function(data,index){
  boot_dat <- data[index,]
  model0 <- glm(wt_delta ~ sex + age + income + sbp + dbp + price71 + tax71 + race, data=subset(boot_dat,qsmk==0), family=binomial("logit"))
  model1 <- glm(wt_delta ~ sex + age + income + sbp + dbp + price71 + tax71 + race, data=subset(boot_dat,qsmk==1), family=binomial("logit"))
  mu1 <- predict(model1,newdata=boot_dat,type="response")
  mu0 <- predict(model0,newdata=boot_dat,type="response")
  
  marg_stand_OR_ <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
  marg_stand_RR_ <- mean(mu1)/mean(mu0)
  marg_stand_RD_ <- mean(mu1)-mean(mu0)
  res <- c(marg_stand_RD_,marg_stand_RR_,marg_stand_OR_)
  return(res)
}

#' Run the boot function. Set a seed to obtain reproducibility
set.seed(123)
boot_res <- boot(nhefs,bootfunc,R=2000)

boot_RD <- boot.ci(boot_res,index=1)
boot_RR <- boot.ci(boot_res,index=2)
boot_OR <- boot.ci(boot_res,index=2)

marg_stand_OR
marg_stand_RR
marg_stand_RD

boot_RD
boot_RR
boot_OR

```


When predicted risks are estimated using a logistic model, relying on marginal standardization will not result in probability estimates outside the bounds [0, 1]. And because the robust variance estimator is not required, model-based standardization will not be as affected by small sample sizes. However, the bootstrap is more computationally demanding than alternative variance estimators, which may pose problems in larger datasets.


# References